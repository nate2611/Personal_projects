{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine Tuning: XGBoost and SVM!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prep of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1358, 54)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "resumen_alcance3=pd.read_csv('deepldataset2.csv')\n",
    "resumen_alcance3=resumen_alcance3.drop(['Unnamed: 0'], axis=1)\n",
    "resumen_alcance3=resumen_alcance3.drop_duplicates()\n",
    "resumen_alcance3.shape\n",
    "dataset_cleaned_eng = pd.read_csv(\"data_cleaned2.csv\")\n",
    "dataset_cleaned_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1358, 55)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned_eng['Resumen_proyecto_eng']=resumen_alcance3['Resumen proyecto']\n",
    "dataset_cleaned_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_noBig= pd.read_csv('data_cleaned3_noBig.csv')\n",
    "dataset_noBig=dataset_noBig.drop(['Unnamed: 0'], axis=1)\n",
    "dataset_noBig=dataset_noBig.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged=pd.merge(dataset_noBig, dataset_cleaned_eng, on=['Fecha inicio', 'Agrupación IT Demanda', 'Gestor Demanda', 'Unidad IT',\n",
    "       'Categoría', 'IT Manager', 'Prioridad', 'Business Owner',\n",
    "       'Agrupación IT de Ejecución', 'Objetivo', 'Informacional',\n",
    "       'Área Peticionaria', 'Resumen proyecto', 'Alcance', 'Horas'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the desired columns\n",
    "selected_columns = ['Resumen_proyecto_eng', 'Horas', 'Fecha inicio']  # Replace with the actual column names\n",
    "\n",
    "# Create a new dataset with selected columns\n",
    "dataset_final = pd.DataFrame()\n",
    "\n",
    "# Copy selected columns\n",
    "for column in selected_columns:\n",
    "    dataset_final[column] = df_merged[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Horas Categories\n",
       "301-800     372\n",
       "801-2000    338\n",
       "0-300       320\n",
       "+2000       258\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the hours in bins\n",
    "\n",
    "bins = [0, 300, 800, 2000, 47000]  \n",
    "labels = ['0-300', '301-800','801-2000','+2000']  \n",
    "target_categories = pd.cut(dataset_final['Horas'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "dataset_final['Horas Categories'] = target_categories\n",
    "dataset_final = dataset_final.drop(columns = [\"Horas\"])\n",
    "dataset_final['Horas Categories'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nathanielthomascopeland/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nathanielthomascopeland/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nathanielthomascopeland/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nathanielthomascopeland/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")  # For Stemming\n",
    "nltk.download(\"wordnet\")  # For Lemmatization\n",
    "nltk.download(\"stopwords\")  # For Stopword Removal\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Import module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create an instance of RegexpTokenizer for alphanumeric tokens\n",
    "preprocessed = []\n",
    "tokeniser = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "for resumen in dataset_final['Resumen_proyecto_eng']:\n",
    "    try:\n",
    "        tokens = tokeniser.tokenize(resumen)\n",
    "        lemmas = [lemmatiser.lemmatize(word.lower(), pos='v') for word in tokens]\n",
    "        key_words = [word for word in lemmas if word not in stopwords.words('english') and len(word) > 2 and not any(char.isdigit() for char in word)]\n",
    "\n",
    "        \n",
    "        # Extract bigrams\n",
    "        #bigram_measures = BigramAssocMeasures()\n",
    "        #finder = BigramCollocationFinder.from_words(key_words)\n",
    "        #bigrams = finder.nbest(bigram_measures.pmi, 15)  # Adjust the number of top bigrams as desired\n",
    "        \n",
    "        preprocessed.append(key_words)\n",
    "    except:\n",
    "        preprocessed.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#double relevance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Step 1: Combine all words into a single list\n",
    "all_words = [word for sublist in preprocessed for word in sublist]\n",
    "\n",
    "# Step 2: Calculate TF-IDF scores\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([\" \".join(all_words)])\n",
    "\n",
    "# Step 3: Define threshold and filter words\n",
    "threshold = 0.02 # Adjust the threshold as desired\n",
    "filtered_words = [word for word, score in zip(all_words, X.toarray()[0]) if score > threshold]\n",
    "\n",
    "# Step 4: Filter the preprocessed list using the filtered words\n",
    "filtered_preprocessed = []\n",
    "for sublist in preprocessed:\n",
    "    filtered_sublist = [word for word in sublist if word in filtered_words]\n",
    "    filtered_preprocessed.append(filtered_sublist)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training: filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1288, 230)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 Bag of Word Modelling\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "count_vec = MultiLabelBinarizer()\n",
    "mlb = count_vec.fit(filtered_preprocessed)\n",
    "df_filtered = pd.DataFrame(mlb.transform(filtered_preprocessed), columns=[mlb.classes_])\n",
    "\n",
    "df_filtered.to_csv('preprocessedbadofwordsresumen2_filtered.csv')\n",
    "df=pd.read_csv('preprocessedbadofwordsresumen2_filtered.csv')\n",
    "\n",
    "df['Fecha inicio'] = pd.to_datetime(dataset_final['Fecha inicio'])\n",
    "df['Horas_bins']=dataset_final['Horas Categories']\n",
    "df=df.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "df=df.drop(columns=['Unnamed: 0'])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoostCategories\n",
    "categories_num=[]\n",
    "for x in df[\"Horas_bins\"]:\n",
    "    if x=='0-300':\n",
    "        categories_num.append(0)\n",
    "    elif x=='301-800':\n",
    "        categories_num.append(1)\n",
    "    elif x=='801-2000':\n",
    "        categories_num.append(2)\n",
    "    elif x=='+2000':\n",
    "        categories_num.append(3)\n",
    "    else:\n",
    "        categories_num.append(None)\n",
    "        \n",
    "df[\"Category numbers\"]=categories_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date= pd.to_datetime(\"2022-04-01\")\n",
    "df_before=df[df['Fecha inicio']<split_date]\n",
    "df_after=df[df['Fecha inicio']>=split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing all the splits\n",
    "y_after=df_after[\"Category numbers\"]\n",
    "X_after=df_after.drop(columns=[\"Category numbers\",\"Horas_bins\", \"Fecha inicio\"])\n",
    "y=df_before[\"Category numbers\"]\n",
    "X=df_before.drop(columns=[\"Category numbers\",\"Horas_bins\", \"Fecha inicio\"])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'pca__n_components': 0.9, 'svm__C': 1, 'svm__gamma': 'scale', 'svm__kernel': 'rbf', 'svm__probability': True, 'svm__shrinking': True}\n",
      "Test Set Score: 0.42\n"
     ]
    }
   ],
   "source": [
    "#Gridsearch SVC Model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "svm_model = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [0.8, 0.9, 0.95],\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['linear', 'rbf'],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'svm__shrinking': [True, False],\n",
    "    'svm__probability': [True, False]\n",
    "}\n",
    "\n",
    "steps = [('pca', pca), ('svm', svm_model)]\n",
    "pipesvm = Pipeline(steps)\n",
    "\n",
    "grid_search = GridSearchCV(pipesvm, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = pipesvm.set_params(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "After_score = best_model.score(X_after, y_after)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
