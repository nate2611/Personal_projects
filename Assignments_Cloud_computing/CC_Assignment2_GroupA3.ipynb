{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a3cb1c",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "\n",
    "**Team A3:** Berta Alguero, John Bergmann, Federico Colombo, Nimit Jain, Nathaniel Thomas-Copeland\n",
    "\n",
    "## Web Scraping Linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb697a",
   "metadata": {},
   "source": [
    "### 1. Requirements\n",
    "Run the next 3 cells only if you don't have the following libraries previously installed:\n",
    "- bs4\n",
    "- selenium\n",
    "- webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e548a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad73b9c",
   "metadata": {},
   "source": [
    "### 2. Setting the environment\n",
    "Definition of fucntions that can make the later for loop shorter, and creation of the lists that will be later filed in with the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e90fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details_or_none(soup_tree, index): \n",
    "    #function used to parse ul trees with identical li tags (aka lists)\n",
    "    \n",
    "    try:\n",
    "        result = soup_tree[index].span.text.strip()\n",
    "    except:\n",
    "        result = None  \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_employees(employee_range): \n",
    "    #function to handle different employee sizes/displays and output them as int\n",
    "\n",
    "    try:\n",
    "        out = \"\"\n",
    "        employees = employee_range.split(\"-\")[1]\n",
    "        \n",
    "        for char in employees:\n",
    "            if char in string.digits:\n",
    "                out += char\n",
    "            \n",
    "        return int(out)\n",
    "    \n",
    "    except:\n",
    "        out = \"\"\n",
    "        \n",
    "        for char in employee_range:\n",
    "            if char in string.digits:\n",
    "                out += char\n",
    "                \n",
    "        return int(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followers(follower_count): \n",
    "    #function to handle different follower sizes/displays and output them as int\n",
    "\n",
    "    out = \"\"\n",
    "\n",
    "    for num in follower_count:\n",
    "        if num in string.digits:\n",
    "            out += num\n",
    "\n",
    "    return int(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bee8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to actually capture results when running the webscraping (by appending new elements).\n",
    "\n",
    "job_ids = []\n",
    "offer_url = []\n",
    "company = []\n",
    "job_title = []\n",
    "location = []\n",
    "state = []\n",
    "posting_date = []\n",
    "num_applicants = []\n",
    "workspace = []\n",
    "seniority = []\n",
    "employment_type = []\n",
    "industry = []\n",
    "application_through_linkedin = []\n",
    "python_required = []\n",
    "promoted = []\n",
    "num_employees = []\n",
    "followers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 user agents(UA) to crawl with since 7 is a magic number and we don't want \"python\" to show up as the UA (to avoid getting blocked)\n",
    "\n",
    "user_agents = [\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.365\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\"\n",
    "               ]\n",
    "\n",
    "user_agent = random.choice(user_agents)\n",
    "opts = Options()\n",
    "opts.add_argument(f\"user-agent={user_agent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4e82d",
   "metadata": {},
   "source": [
    "### 3. Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a new tab and go to Linkedin\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options = opts)\n",
    "\n",
    "driver.maximize_window() \n",
    "driver.minimize_window() \n",
    "driver.switch_to.window(driver.current_window_handle)\n",
    "driver.implicitly_wait(10)\n",
    "driver.maximize_window() \n",
    "\n",
    "driver.get(\"https://www.linkedin.com/jobs\")\n",
    "\n",
    "# Job Query\n",
    "\n",
    "search_field = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='keywords']\")))\n",
    "location_field = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='location']\")))\n",
    "\n",
    "search_field.clear()\n",
    "location_field.clear()\n",
    "\n",
    "search_input = \"data scientist, barcelona\"\n",
    "location_input = \"Barcelona, Catalonia, Spain\"\n",
    "time.sleep(2)\n",
    "\n",
    "search_field.send_keys(search_input)\n",
    "location_field.send_keys(location_input)\n",
    "location_field.submit()\n",
    "time.sleep(2)\n",
    "\n",
    "# Log In\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"a[data-tracking-control-name='public_jobs_nav-header-signin']\"))).click()\n",
    "\n",
    "username_field = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"username\")))\n",
    "password_field = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"password\")))\n",
    "\n",
    "username_field.clear()\n",
    "password_field.clear()\n",
    "\n",
    "username = \"packetlosspony@gmail.com\"\n",
    "password = \"heqbi7-xEgvem-suhsem\"\n",
    "time.sleep(2)\n",
    "\n",
    "username_field.send_keys(username)\n",
    "password_field.send_keys(password)\n",
    "password_field.submit()\n",
    "time.sleep(2)\n",
    "\n",
    "results = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, \"full-width.artdeco-entity-lockup__title.ember-view\")))\n",
    "\n",
    "\n",
    "# Scroll down once to define page_range\n",
    "\n",
    "time.sleep(2)\n",
    "job_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "last_page = int(job_soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")[-1].text.strip())\n",
    "current_page = 1\n",
    "page_range = range(current_page, last_page + 1) #pythonic indexing so our tactic works :D\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts scraping\n",
    "\n",
    "for page in tqdm(page_range, desc = \"Overall Progress\"):\n",
    "    time.sleep(2)\n",
    "    \n",
    "    scroll_times = range(6)\n",
    "    for x in scroll_times:\n",
    "        results = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, \"full-width.artdeco-entity-lockup__title.ember-view\")))\n",
    "        driver.execute_script(\"return arguments[0].scrollIntoView(true);\", results[-1])\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    results = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, \"full-width.artdeco-entity-lockup__title.ember-view\")))\n",
    "    print(len(results))\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    time.sleep(2)\n",
    "    job_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    job_list = job_soup.find(\"ul\", {\"class\": \"scaffold-layout__list-container\"}).find_all(\"li\", {\"id\": re.compile(\"^ember\")})\n",
    "\n",
    "    for job in tqdm(job_list, desc = f\"Scraping page {current_page}\", leave = False):\n",
    "        \n",
    "        job_id = job[\"data-occludable-job-id\"]\n",
    "        job_ids.append(job_id)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Retrieving urls \n",
    "        \n",
    "        if job.find(\"a\", {\"class\": \"disabled ember-view job-card-container__link job-card-list__title\"}) == None:\n",
    "            url = None\n",
    "        else:\n",
    "            url = job.find(\"a\", {\"class\": \"disabled ember-view job-card-container__link job-card-list__title\"})[\"href\"]\n",
    "            url = (\"https://www.linkedin.com\" + url) # Make sure we get english webpage\n",
    "        offer_url.append(url)\n",
    "        \n",
    "        # Clicking on the next individual job offer\n",
    "        \n",
    "        clicker = results[count]\n",
    "        time.sleep(1)\n",
    "        clicker.click()\n",
    "        count += 1\n",
    "        time.sleep(2)\n",
    "        \n",
    "\n",
    "        # Retrieving initial information from the job offer\n",
    "        \n",
    "        try:\n",
    "            company_name = job.find(\"a\", {\"class\": \"job-card-container__link job-card-container__company-name ember-view\"}).text.strip()\n",
    "            company.append(company_name) #extra variable needed for later\n",
    "        except:\n",
    "            company.append(None)\n",
    "\n",
    "        try:\n",
    "            job_title.append(job.find(\"a\", {\"class\": \"disabled ember-view job-card-container__link job-card-list__title\"}).text.strip())\n",
    "        except:\n",
    "            job_title.append(None)\n",
    "\n",
    "        try:\n",
    "            location.append(job.find(\"li\", {\"class\": \"job-card-container__metadata-item\"}).text.strip())\n",
    "        except:\n",
    "            location.append(None)\n",
    "\n",
    "        try:\n",
    "            workspace.append(job.find(\"li\", {\"class\": \"job-card-container__metadata-item job-card-container__metadata-item--workplace-type\"}).text.strip())\n",
    "        except:\n",
    "            workspace.append(None)\n",
    "\n",
    "        try:\n",
    "            if job.find(\"li\", {\"class\": \"t-12 t-normal t-black--light job-card-container__footer-item\"}).text.strip():\n",
    "                promoted.append(True)\n",
    "        except:\n",
    "            promoted.append(False)\n",
    "\n",
    "        \n",
    "        # Scroll down within the inidivdual job offer page\n",
    "        \n",
    "        scroll_detail_tag = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"SALARY\"]')))\n",
    "        \n",
    "        driver.execute_script(\"return arguments[0].scrollIntoView(true);\", scroll_detail_tag)\n",
    "        time.sleep(2)\n",
    "        job_details = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        \n",
    "        # Retrieving some more information from the job offer\n",
    "        \n",
    "        try:\n",
    "            num_applicants.append(int(job_details.find(\"span\", {\"class\": \"jobs-unified-top-card__applicant-count\"}).text.replace(\"applicants\",\"\").strip()))\n",
    "        except:\n",
    "            num_applicants.append(0) ###rare but had occasion with nothing here if \"over 200\" 200, except:None or 0\n",
    "\n",
    "        try:\n",
    "            posting_date.append(job_details.find(\"span\", {\"class\": \"jobs-unified-top-card__posted-date\"}).text.strip())\n",
    "        except:\n",
    "            posting_date.append(None)\n",
    "\n",
    "        \n",
    "        # Some elements share the same tag and class, so we need to parse deeper\n",
    "        \n",
    "        job_criteria = job_details.find_all(\"li\", {\"class\": \"jobs-unified-top-card__job-insight\"})\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for each in job_criteria:\n",
    "            if each.find(\"li-icon\") == None:\n",
    "                continue\n",
    "            \n",
    "            # Finding same name tags using the presence of their icons\n",
    "            \n",
    "            # Employment Type and Seniority\n",
    "            if job_details.find(\"li-icon\", {\"type\" : \"job\"}) is not None:\n",
    "                if each.find(\"li-icon\").find(\"path\") == job_details.find(\"li-icon\", {\"type\" : \"job\"}).find(\"path\"):\n",
    "                    temp = each.text.strip().split(\"·\")\n",
    "                  \n",
    "                    # Sometimes there is salary too making the list len 3 so we have to take the second last element for employment type\n",
    "                    if len(temp) == 2:\n",
    "                        employment_type.append(temp[0].strip())\n",
    "                        seniority.append(temp[-1].strip())\n",
    "                    elif len(temp) == 3:\n",
    "                        employment_type.append(temp[-2].strip())\n",
    "                        seniority.append(temp[-1].strip())\n",
    "                    else:\n",
    "                        employment_type.append(temp[-1].strip())\n",
    "                        seniority.append(None)\n",
    "            \n",
    "            else:\n",
    "                employment_type.append(None)\n",
    "                seniority.append(None)            \n",
    "                \n",
    "             \n",
    "            # Number of employees and Industry\n",
    "            if job_details.find(\"li-icon\", {\"type\" : \"company\"}) is not None:\n",
    "                if each.find(\"li-icon\") == job_details.find(\"li-icon\", {\"type\" : \"company\"}):\n",
    "                    temp = each.text.strip().split(\"·\")\n",
    "                    \n",
    "                    if len(temp) == 1:\n",
    "                        num_employees.append(int(\"\".join(re.findall(r'\\d+',temp[0].split(\" \")[0].strip().split(\"-\")[-1]))))\n",
    "                        industry.append(None)\n",
    "                    elif len(temp) == 2:\n",
    "                        num_employees.append(int(\"\".join(re.findall(r'\\d+',temp[0].split(\" \")[0].strip().split(\"-\")[-1]))))\n",
    "                        industry.append(temp[1].strip())\n",
    "            else:\n",
    "                num_employees.append(None)\n",
    "                industry.append(None)\n",
    "                \n",
    "            \n",
    "            # State\n",
    "            if job_details.find(\"li-icon\", {\"class\" : \"ivm-view-attr__icon--signal-positive\"}) is not None:\n",
    "                if each.find(\"li-icon\") == job_details.find(\"li-icon\", {\"class\" : \"ivm-view-attr__icon--signal-positive\"}):\n",
    "                    store = each.text.strip()\n",
    "                   \n",
    "                    if store in \"Actively recruiting\":\n",
    "                        status = \"On-going\"\n",
    "                    elif store in \"Be an early applicant\":\n",
    "                        status = \"Early applicantions\"\n",
    "                    else:\n",
    "                        status = \"Others\"\n",
    "            else:\n",
    "                status = None\n",
    "            \n",
    "        state.append(status)\n",
    "                    \n",
    "\n",
    "        # Application trhough Linkeidn\n",
    "        which_apply = job_details.find(\"span\", {\"class\": \"artdeco-button__text\"}).text.strip()\n",
    "\n",
    "        if which_apply == \"Easy Apply\":\n",
    "            application_through_linkedin.append(True)\n",
    "        else:\n",
    "            application_through_linkedin.append(False)\n",
    "\n",
    "        \n",
    "        # Python Requirement\n",
    "        job_description = job_details.find(\"div\", {\"id\": \"job-details\"}).find(\"span\").text.strip()\n",
    "\n",
    "        if re.compile(r\"\\b({0})\\b\".format(\"python\"), flags = re.IGNORECASE).search(job_description): #check for the actual word 'python' (case un-sensitive), not just a sequence of strings. Who knows, someone might be looking for an employee experienced with handling 'Pythonidae' snakes :D \n",
    "            python_required.append(True)\n",
    "        else:\n",
    "            python_required.append(False)\n",
    "\n",
    "        # Number of Followers\n",
    "        try:\n",
    "            follower_num = job_details.find(\"div\", {\"class\": \"artdeco-entity-lockup__subtitle ember-view t-16\"}).text.strip()\n",
    "            followers.append(int(get_followers(follower_num)))\n",
    "        except:\n",
    "            followers.append(None)\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Change onto the next page in the results\n",
    "    \n",
    "    current_page += 1\n",
    "    \n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, f\"button[aria-label = 'Page {current_page}']\"))).click()\n",
    "    \n",
    "    \n",
    "print(\"Done :D\") # End of loop - end of scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53dad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Scraping tab\n",
    "\n",
    "driver.quit() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f127c56",
   "metadata": {},
   "source": [
    "### 4. Store all the data into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Job Title\": job_title,\n",
    "                   \"Company Name\": company,\n",
    "                   \"Location\": location,\n",
    "                   \"State\": state,\n",
    "                   \"Posting Date\": posting_date,\n",
    "                   \"Offer URL\": offer_url,\n",
    "                   \"Number of Applicants\": num_applicants,\n",
    "                   \"Promoted\": promoted,\n",
    "                   \"Workspace\": workspace,\n",
    "                   \"Seniority\": seniority,\n",
    "                   \"Employment Type\": employment_type,\n",
    "                   \"Industry\": industry,\n",
    "                   \"Python Required\": python_required,\n",
    "                   \"Application through Linkedin\": application_through_linkedin,\n",
    "                   \"Number of Employees\": num_employees,\n",
    "                   \"Followers\": followers\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
